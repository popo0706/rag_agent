# ------------------------------------------------------------------------------
# ğŸ”° ä»Šå›ã®ãƒã‚¤ãƒ³ãƒˆãƒ»ç”¨èªã¾ã¨ã‚
# ãƒ»RAGï¼ˆRetrieval-Augmented Generationï¼‰
#     â”” æ¤œç´¢ã§å–ã‚Šå‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¤–éƒ¨çŸ¥è­˜ï¼‰ã‚’ LLM ã®è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã—ã¦å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹æ‰‹æ³•
# ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥
#     â”” ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ API ã‚³ãƒ¼ãƒ«ã®å›æ•°ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€ä¸€åº¦å–ã‚Šå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã™ã‚‹ä»•çµ„ã¿
# ãƒ»pickle
#     â”” Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãã®ã¾ã¾ãƒã‚¤ãƒŠãƒªåŒ–ã—ã¦ä¿å­˜ï¼å¾©å…ƒã§ãã‚‹æ¨™æº–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆâ€»ä¿¡é ¼ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ï¼‰
# ãƒ»LangChain / LangSmith
#     â”” LangChain ã¯ LLM ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã® OSSã€LangSmith ã¯ãã®å®Ÿé¨“ç®¡ç† SaaS
# ãƒ»ragas
#     â”” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ QA ç”¨ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ------------------------------------------------------------------------------

"""
GitHub ä¸Šã® LangChain ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ `.mdx` ã ã‘ã‚’èª­ã¿è¾¼ã¿ã€
ãƒ‡ãƒ¼ã‚¿ã‚’ pickle ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¤ã¤ ragas ã§ QA ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€
æœ€çµ‚çš„ã« LangSmith ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
åˆå›å®Ÿè¡Œã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ãŒç™ºç”Ÿã—ã€ï¼’å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†åˆ©ç”¨ã™ã‚‹ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ QAï¼ˆï¼RAG ã‚·ã‚¹ãƒ†ãƒ ï¼‰ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¥ãã‚Šã‚’è‡ªå‹•åŒ–ã§ãã‚‹ã€‚
"""

# ===== ã“ã“ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
# OS ä¾å­˜ã®ãƒ‘ã‚¹æ“ä½œã‚„ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
import os

# Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒã‚¤ãƒŠãƒªå½¢å¼ã§ä¿å­˜ï¼å¾©å…ƒã™ã‚‹
import pickle

# Git ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ LangChain ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å®Ÿè£…
from langchain_community.document_loaders import GitLoader

# Jupyter ãªã©ã§æ—¢ã«èµ°ã£ã¦ã„ã‚‹ asyncio ãƒ«ãƒ¼ãƒ—ã¨ç«¶åˆã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãƒ‘ãƒƒãƒ
import nest_asyncio

# ragas ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆä½œæˆãƒ„ãƒ¼ãƒ«
from ragas.testset.generator import TestsetGenerator

# ãƒ†ã‚¹ãƒˆå•é¡Œã® â€œç¨®é¡â€ ã‚’ä½œã‚‹ãŸã‚ã®é€²åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
from ragas.testset.evolutions import simple, reasoning, multi_context

# OpenAI ã®ãƒãƒ£ãƒƒãƒˆ LLM ã¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# LangSmith ã¸ãƒ‡ãƒ¼ã‚¿ã‚’é€ã‚‹ SDK
from langsmith import Client

# ===== ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµ‚ã‚ã‚Š =====


# ------------------------------------------------------------------------------
# æ‹¡å¼µå­ãŒ .mdx ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’å–ã‚Šè¾¼ã‚€ãŸã‚ã®ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°
# GitLoader ã« callback ã¨ã—ã¦æ¸¡ã™ã¨ã€True ã‚’è¿”ã™ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
# ------------------------------------------------------------------------------
def file_filter(file_path: str) -> bool:
    """
    .mdx ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’é¸ã³å‡ºã™ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

    Parameters
    ----------
    file_path : str
        GitLoader ã‹ã‚‰æ¸¡ã£ã¦ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹

    Returns
    -------
    bool
        æ‹¡å¼µå­ãŒ .mdx ãªã‚‰ Trueï¼ˆ= å–ã‚Šè¾¼ã‚€ï¼‰ï¼ãã‚Œä»¥å¤–ã¯ False

    Notes
    -----
    GitLoader å´ãŒã€Œã“ã®é–¢æ•°ãŒ True ã‚’è¿”ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚€ã€ã¨ã„ã†è¨­è¨ˆã€‚
    æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯ã¯ endswith ã§ååˆ†è»½é‡ã€‚(å‚è€ƒ: str.endswith)
    """
    return file_path.endswith(".mdx")


# ------------------------------------------------------------------------------
# pickle ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ããƒ‘ã‚¹
# ------------------------------------------------------------------------------
cache_path = "document.pkl"

# ------------------------------------------------------------------------------
# â‘  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡ã‘ã‚Œã° GitHub ã‹ã‚‰ clone â†’ èª­ã¿è¾¼ã¿ â†’ pickle ä¿å­˜
# â‘¡ ã‚ã‚Œã° pickle ã‹ã‚‰å³æ™‚ãƒ­ãƒ¼ãƒ‰
# ------------------------------------------------------------------------------
if not os.path.exists(cache_path):
    # GitLoader ã¯ clone â†’ checkout â†’ ãƒ•ã‚¡ã‚¤ãƒ«èµ°æŸ»ã¾ã§ä¸€æ‹¬ã§ã‚„ã£ã¦ãã‚Œã‚‹
    loader = GitLoader(
        clone_url="https://github.com/langchain-ai/langchain",
        repo_path="./langchain",  # clone å…ˆã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ«ãƒ€
        branch="master",
        file_filter=file_filter,  # .mdx ã ã‘èª­ã‚€ã‚ˆã†ã«è¨­å®š
    )
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ I/O ãŒç™ºç”Ÿã™ã‚‹é‡ã„å‡¦ç†
    documents = loader.load()

    # ä¸€åº¦å–å¾—ã—ãŸã‚‰ pickle ã§ä¿å­˜ã—ã¦æ¬¡å›ã‹ã‚‰é«˜é€ŸåŒ–
    with open(cache_path, "wb") as f:
        # ãªãœ wb ã‹? â€• ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§æ›¸ãè¾¼ã¿(write binary)ã™ã‚‹ãŸã‚
        pickle.dump(documents, f)
else:
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨æ™‚ã¯ä¸€ç¬ã§ãƒ­ãƒ¼ãƒ‰å®Œäº†
    with open(cache_path, "rb") as f:
        documents = pickle.load(f)

print(len(documents))  # ã©ã®ãã‚‰ã„ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ‰±ã†ã‹å¯è¦–åŒ–ã—ã¦ãŠã


# ------------------------------------------------------------------------------
# LangSmith ã§å¾Œå‡¦ç†ã—ã‚„ã™ã„ã‚ˆã†ã€metadata ã« 'filename' ã‚­ãƒ¼ã‚’è¶³ã™
# ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å¾Œã§æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ã—ãŸã„ã¨ãã«ä¾¿åˆ©ï¼‰
# ------------------------------------------------------------------------------
for document in documents:
    # GitLoader ãŒä»˜ä¸æ¸ˆã¿ã® "source" ã‚’ "filename" ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ã ã‘
    document.metadata["filename"] = document.metadata["source"]


# ------------------------------------------------------------------------------
# ragas ã§ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ä½œã‚‹æº–å‚™
# Jupyter ã§ãƒã‚¹ãƒˆã—ãŸã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒã‚ã‚‹ã¨æ€’ã‚‰ã‚Œã‚‹ã®ã§å…ˆã«ãƒ‘ãƒƒãƒ
# ------------------------------------------------------------------------------
nest_asyncio.apply()

# ------------------------------------------------------------------------------
# TestsetGenerator ã®æ§‹ç¯‰
# â”€ generator_llm : ãƒ†ã‚¹ãƒˆå•é¡Œï¼ˆè³ªå•ï¼‰ã‚’ç”Ÿæˆã™ã‚‹ LLM
# â”€ critic_llm    : è³ªã®ä½ã„è³ªå•ã‚’å¼¾ã LLM
# â”€ embeddings    : ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æ¤œç´¢ã«ä½¿ã†åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
# ------------------------------------------------------------------------------
generator = TestsetGenerator.from_langchain(
    generator_llm=ChatOpenAI(model="gpt-4.1-nano"),
    critic_llm=ChatOpenAI(model="gpt-4.1-nano"),
    embeddings=OpenAIEmbeddings(),
)

# ------------------------------------------------------------------------------
# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é£Ÿã‚ã›ã¦ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’è‡ªå‹•ç”Ÿæˆ
# distributions ã§ã€Œã©ã®ã‚¿ã‚¤ãƒ—ã®å•é¡Œã‚’ä½•å‰²ã¤ãã‚‹ã‹ã€ã‚’æŒ‡å®šã—ã¦ã„ã‚‹
# ------------------------------------------------------------------------------
testset = generator.generate_with_langchain_docs(
    documents,
    test_size=4,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},
    raise_exceptions=True,  # ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„ã‚ˆã†é€”ä¸­å¤±æ•—ã§å³ä¾‹å¤–
)

# DataFrame ã§ä¸­èº«ã‚’ã–ã£ã¨ç¢ºèª (pandas ã«å¤‰æ›ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ãŒæ¨™æº–è£…å‚™)
df = testset.to_pandas()
print(df)


# ------------------------------------------------------------------------------
# LangSmith ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
# LangSmith ã¯ã€Œå…¥åŠ›(inputs)ã€ã€Œå‡ºåŠ›(outputs)ã€ã€Œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(metadata)ã€ã®
# ï¼“ã‚«ãƒ©ãƒ ã§ Examples ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã€ã¾ãšãã‚Œãã‚Œã®ãƒªã‚¹ãƒˆã‚’ä½œã‚‹
# ------------------------------------------------------------------------------
dataset_name = "agent-book"
client = Client()  # ç’°å¢ƒå¤‰æ•° LANGCHAIN_API_KEY ç­‰ã§è‡ªå‹•èªè¨¼

# åŒåãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ—¢ã«ã‚ã‚Œã°å‰Šé™¤ã—ã¦ä½œã‚Šç›´ã—ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
if client.has_dataset(dataset_name=dataset_name):
    client.delete_dataset(dataset_name=dataset_name)
dataset = client.create_dataset(dataset_name=dataset_name)

# LangSmith ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©°ã‚æ›¿ãˆã‚‹
inputs, outputs, metadatas = [], [], []
for record in testset.test_data:
    # ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã¯ã€Œè³ªå•ã€ã®ã¿
    inputs.append({"question": record.question})

    # ãƒ¢ãƒ‡ãƒ«ãŒè¿”ã™ã¹ãç­”ãˆï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨æ­£è§£ï¼‰ã‚’ outputs ã«
    outputs.append(
        {
            "contexts": record.contexts,
            "ground_truth": record.ground_truth,
        }
    )

    # è¿½åŠ æƒ…å ±ã¨ã—ã¦ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¨å•é¡Œã‚¿ã‚¤ãƒ—ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¸
    metadatas.append(
        {
            "source": record.metadata[0]["source"],
            "evolution_type": record.evolution_type,
        }
    )

# ä¸€æ‹¬ã§ Example ã‚’ç™»éŒ²
client.create_examples(
    inputs=inputs,
    outputs=outputs,
    metadata=metadatas,
    dataset_id=dataset.id,
)
